**LLmHub** is the dropâ€‘in API that decides which Large Language Model should answer each promptâ€”so you donâ€™t have to. Slash inference bills, speed up responses, and keep full control over quality & privacy.

## âœ¨ Why LLmHub?

* â±ï¸ **Latency-aware routing** â€“ chooses the fastest model available in realâ€‘time.
* ğŸ’¸ **Cost optimiser** â€“ automatically sends simple prompts to cheaper models, reserving GPTâ€‘4oâ€‘class power for complex tasks.
* ğŸ“Š **Transparent analytics** â€“ detailed perâ€‘request breakdowns of tokens, spend and speed.
* ğŸ”’ **Bringâ€‘yourâ€‘ownâ€‘keys** â€“ your credentials never leave your VPC; works with OpenAI, Anthropic, Mistral, TogetherÂ AI and more.
* ğŸ§  **Learning cache** â€“ semantic caching means repeated or similar prompts return instantly at zero cost.
* âš™ï¸ **Plugâ€‘andâ€‘play SDKs** â€“ firstâ€‘class clients for Python, JavaScript/TypeScript and Go.

## ğŸš€ Quick Start

```bash
pip install llmhub
export LLMHUB_API_KEY="YOUR_KEY"
```

```python
from llmhub import Client

client = Client()
response = client.chat("Summarise the latest AI news in 3 bullet points.")
print(response.text)
```

## ğŸ› ï¸ Features

| Category                      | What you get                                                              |
| ----------------------------- | ------------------------------------------------------------------------- |
| **Automatic Model Selection** | Realâ€‘time scoring based on price, speed, capacity and historical accuracy |
| **Smart Retry & Fallback**    | If a provider throttles, LLmHub transparently reâ€‘routes the call          |
| **Version Pinning**           | Lock a route to a specific provider/model for compliance needs            |
| **Perâ€‘org Limits**            | Define budgets, rate limits and hard caps in a few clicks                 |

## ğŸ—ï¸ API Key Management

Generate and rotate API keys from the dashboard or via CLI:

```bash
llmhub keys create "dev-environment"
llmhub keys list
llmhub keys revoke <key_id>
```

Every key is scoped and can be limited by daily spend, QPS or model list.

## ğŸ“Š Realâ€‘time Metrics

The dashboard shows:

* Requests per minute & latency percentiles
* Token usage by provider
* Cost heatâ€‘map over time
* Cache hit ratio

## ğŸ¤ Contributing

We love community contributions!  Whether itâ€™s a bug fix, new provider adapter or docs improvement:

1. **Fork** the repo & create a feature branch.
2. **Commit** your changes with clear messages.
3. **Open** a Pull Request.
4. Weâ€™ll run CI and help you land it.

See [`CONTRIBUTING.md`](CONTRIBUTING.md) for coding standards & release flow.

## ğŸ›£ï¸ Roadmap

* ğŸ”Œ WebSocket streaming responses
* ğŸ›¡ï¸ Onâ€‘premise â€œairâ€‘gappedâ€ deployment option
* ğŸ“ˆ Predictive preâ€‘warm logic for even lower p99 latency

Vote or propose new features in [Discussions](https://github.com/llmhub/llmhub/discussions).

## ğŸ“ Contact

| Channel      | Link                                                                       |
| ------------ | -------------------------------------------------------------------------- |
| ğŸŒ Website   | [https://llmhub.dev](https://llmhub.dev)                                   |
| âœ‰ï¸ Email     | [prateek@llmhub.dev](mailto:prateek@llmhub.dev)                            |
| ğŸ’¼ LinkedIn  | [https://linkedin.com/company/llmhub](https://linkedin.com/company/llmhub) |
| ğŸ¦ Twitter/X | @llmhubdev                                                                 |

## ğŸ“ License

Distributed under the MIT License. See [`LICENSE`](LICENSE) for more information.

---

<p align="center">Made with â¤ï¸ by the LLmHub team</p>
